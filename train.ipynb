{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot made with Python\n",
    "\n",
    "This is my(Adheesh Trivedi's) implementation of chatbot made in python programming language and libraries such as `numpy`, `nltk` (Natural Language Tool-kit), `tensorflow` (The Giant Machine Learning library) and few other.\n",
    "```asciidoc\n",
    "Creator     :: Adheesh Trivedi\n",
    "Class       :: 12th\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from nltk.stem.lancaster import LancasterStemmer\n",
    "    from nltk import word_tokenize\n",
    "    import numpy as np\n",
    "    import pickle as pkl\n",
    "    from json import load\n",
    "    from typing import List, Tuple\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout\n",
    "    from tensorflow.keras.optimizers import SGD\n",
    "except ModuleNotFoundError as err:\n",
    "    print(\"Module named \" + err.name + \" was not found in the path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining some globals\n",
    "\n",
    "* `stemmer` :\\\n",
    "    It's a `LancasterStemmer` instance that is gonna help us classify `word`, `words`, or `wording` as word, or simply say, will remove the tenses part or any preffix or suffix in a word that dosn't change the meaning of the word much.\n",
    "* `intents` :\\\n",
    "    It's a list of dictionaries that is going to have chatbot's all `intents`, `patterns` (The list of patterns of statements that falls under that intent), and a list of responses in the following pattern:\n",
    "    ```json\n",
    "        {\"intent\": \"farewell\",\n",
    "        \"patterns\": [\"goodbye\",\n",
    "        \"goodbyeya\",\n",
    "        \"gtg\",\n",
    "        \"gotta go\",\n",
    "        \"got to go\",\n",
    "        \"cya\",\n",
    "        \"see you\",\n",
    "        \"see ya later\",\n",
    "        \"talk to you later\",\n",
    "        \"gonna go know\",\n",
    "        \"Till next time\"],\n",
    "        \"responses\": [\"Goodbye!\",\n",
    "        \"Alright, have a great time.\",\n",
    "        \"Okay, You can close me.\",\n",
    "        \"It would have been better if you were with me for a some more time. Anyways see you asap.']}\n",
    "    ```\n",
    "* `words` :\\\n",
    "    This is a set of words that chatbot's intents recognise.\n",
    "* `classes` :\\\n",
    "    This is a list of classes (Intent names). We will use this to guess the output given from tensor-flow model.\n",
    "* `docs` :\\\n",
    "    This will hold the dataset or training data.\n",
    "* `ign_lets` :\\\n",
    "    This is a tuple of letter to ignore while we make our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer                 = LancasterStemmer()\n",
    "#stemmer.stem(\"saying\") => \"say\"\n",
    "intents : List[dict]    = load(open('./data/intents.json'))\n",
    "words                   = set()\n",
    "classes : List[str]     = []\n",
    "docs : List[Tuple[List[str], str]] = []\n",
    "ign_lets                = ('?','!',',','.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataset\n",
    "We will loop through all intents and for each intent we will loop through all patterns that belongs to the intent. We will tokenize and then stem the words in the patterns and then add them to `words` and `docs`. `words` is a set of stemmed words so we add the stemmed word to it and for `docs` variable, we make a list of stemmed words for each pattern and create a tuple out of it where first element of the tuple becomes the list of words and the second element is the `class` or say `intent name` that perticular pattern belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for intent in intents:\n",
    "    for pattern in intent['patterns']:\n",
    "        tokens = word_tokenize(pattern)\n",
    "        stemmed_tokens = []\n",
    "        for word in tokens:\n",
    "            word = stemmer.stem(word.lower())\n",
    "            if word not in ign_lets:\n",
    "                words.add(word)\n",
    "                stemmed_tokens.append(word)\n",
    "        docs.append((stemmed_tokens, intent['intent']))\n",
    "    if not intent['intent'] in classes:\n",
    "        classes.append(intent['intent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training data\n",
    "\n",
    "Computer understands 0's and 1's easily so we convert words and classes to a bag of words and list containing 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes   = sorted(list(classes))\n",
    "words     = sorted(list(words))\n",
    "training  = []\n",
    "out_empty = [0] * len(classes)\n",
    "\n",
    "for doc in docs:\n",
    "    bag : List[int] = []\n",
    "    for word in words:\n",
    "        bag.append(1) if word in doc[0] else bag.append(0)\n",
    "\n",
    "    out_row = out_empty[:] # Here we make a copy\n",
    "    out_row[classes.index(doc[1])] = 1\n",
    "    training.append((bag, out_row))\n",
    "\n",
    "# Shuffling the training data\n",
    "training = np.array(training, dtype=tuple)\n",
    "np.random.shuffle(training)\n",
    "train_x, train_y = list(training[:, 0]), list(training[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating model\n",
    "Creating a new Sequential model where we add layers and then feed the training data to the model and then train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Tensor-Bot\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 125)               31375     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 125)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 86)                10836     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 86)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 66)                5742      \n",
      "=================================================================\n",
      "Total params: 47,953\n",
      "Trainable params: 47,953\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(\n",
    "    layers = [\n",
    "        Dense(125, 'relu', input_shape = (len(words),)),\n",
    "        Dropout(0.5, input_shape = (len(words),)),\n",
    "        Dense(86, 'relu', input_shape = (len(words),)),\n",
    "        Dropout(0.5, input_shape = (len(words),)),\n",
    "        Dense(len(classes), 'softmax')\n",
    "    ],\n",
    "    name = \"Tensor-Bot\"\n",
    ")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(SGD(learning_rate = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True),\n",
    "    'categorical_crossentropy', ['accuracy']\n",
    ")\n",
    "\n",
    "hist = model.fit(train_x, train_y, epochs = 225, batch_size = 5, verbose = 0)\n",
    "model.save('data/TensorBot_v2_.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01687764, 0.04219409, 0.05063291, 0.06751055, 0.06329114,\n",
       "       0.08860759, 0.08860759, 0.12658228, 0.15189873, 0.16455697,\n",
       "       0.17299578, 0.24050634, 0.21097046, 0.23628692, 0.27004218,\n",
       "       0.33333334, 0.33755276, 0.40928271, 0.40084389, 0.35021096,\n",
       "       0.4135021 , 0.48101267, 0.50210971, 0.48945147, 0.52320677,\n",
       "       0.54008436, 0.52742618, 0.55696201, 0.64135021, 0.60759491,\n",
       "       0.5527426 , 0.62869197, 0.61181432, 0.70042193, 0.66244727,\n",
       "       0.72151899, 0.70042193, 0.67510551, 0.70042193, 0.6919831 ,\n",
       "       0.69620252, 0.70886075, 0.71308017, 0.75105482, 0.7763713 ,\n",
       "       0.71729958, 0.75949365, 0.75105482, 0.78059071, 0.71729958,\n",
       "       0.73839664, 0.80590719, 0.79746836, 0.78481013, 0.8101266 ,\n",
       "       0.76793247, 0.8101266 , 0.81856543, 0.83544302, 0.84388185,\n",
       "       0.83966243, 0.8101266 , 0.80590719, 0.84388185, 0.83122361,\n",
       "       0.79746836, 0.83966243, 0.82700419, 0.82700419, 0.82700419,\n",
       "       0.81856543, 0.84810126, 0.78481013, 0.85654008, 0.8607595 ,\n",
       "       0.82278478, 0.80168778, 0.8607595 , 0.86497891, 0.84388185,\n",
       "       0.84388185, 0.85654008, 0.86497891, 0.85654008, 0.85232067,\n",
       "       0.86497891, 0.79746836, 0.8607595 , 0.86497891, 0.86919832,\n",
       "       0.89029539, 0.87763715, 0.89029539, 0.88185656, 0.8607595 ,\n",
       "       0.85232067, 0.86919832, 0.87341774, 0.88607597, 0.83966243,\n",
       "       0.87763715, 0.86497891, 0.82278478, 0.86497891, 0.88607597,\n",
       "       0.86497891, 0.84388185, 0.90295357, 0.8607595 , 0.85232067,\n",
       "       0.88607597, 0.84810126, 0.92405063, 0.86919832, 0.84810126,\n",
       "       0.88185656, 0.87763715, 0.84388185, 0.8607595 , 0.90295357,\n",
       "       0.8607595 , 0.8607595 , 0.85232067, 0.89451474, 0.90295357,\n",
       "       0.86919832, 0.85232067, 0.87763715, 0.88607597, 0.89873415,\n",
       "       0.90295357, 0.86919832, 0.87341774, 0.87341774, 0.87341774,\n",
       "       0.89451474, 0.89029539, 0.89029539, 0.9156118 , 0.90295357,\n",
       "       0.85232067, 0.85232067, 0.86497891, 0.89451474, 0.86497891,\n",
       "       0.86919832, 0.87763715, 0.89451474, 0.90717298, 0.86919832,\n",
       "       0.88185656, 0.89873415, 0.89451474, 0.87763715, 0.90295357,\n",
       "       0.88185656, 0.90717298, 0.90295357, 0.89873415, 0.90717298,\n",
       "       0.89029539, 0.83966243, 0.90295357, 0.88607597, 0.89451474,\n",
       "       0.89029539, 0.91139239, 0.86497891, 0.90295357, 0.90717298,\n",
       "       0.89873415, 0.87341774, 0.91139239, 0.8607595 , 0.87763715,\n",
       "       0.90295357, 0.84810126, 0.91139239, 0.90717298, 0.90295357,\n",
       "       0.89873415, 0.88185656, 0.85654008, 0.89873415, 0.87341774,\n",
       "       0.89873415, 0.90717298, 0.91139239, 0.89451474, 0.86919832,\n",
       "       0.87763715, 0.88607597, 0.89873415, 0.91139239, 0.89873415,\n",
       "       0.86919832, 0.9156118 , 0.90717298, 0.89873415, 0.89451474,\n",
       "       0.88607597, 0.92405063, 0.88607597, 0.88185656, 0.91139239,\n",
       "       0.89873415, 0.92405063, 0.91139239, 0.89029539, 0.92827004,\n",
       "       0.92827004, 0.88607597, 0.91139239, 0.89451474, 0.89873415,\n",
       "       0.93670887, 0.90717298, 0.90717298, 0.90717298, 0.91983122,\n",
       "       0.9156118 , 0.88607597, 0.86497891, 0.91983122, 0.9156118 ])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(hist.history['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving some side data usefull for predicting the output\n",
    "We will save `words` and `classes` in a pkl (pickle) file. This will prevent ous from recalculating the data when we predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump((words, classes), open('data/chatbot_dump_v2_.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ec66bbe782f389edb8615b95e71913b48001538f92dc4d53a6c71a0eff16772"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit (system)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
